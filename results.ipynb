{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparing LLM generated questions with OTDB questions\n",
    "\n",
    "This project examines how static quiz questions from the Open Trivia Database (OTDB) differ from dynamically generated questions from a Large Language Model (LLM). Through systematic comparison, the goal is to determine whether LLMs can be a useful supplement or alternative to existing quiz databases."
   ],
   "id": "3eccb7c4621c81eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Categories and Questions from OTDB",
   "id": "e0553c2b711eef08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "from file_utils import load_questions_from_file\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "\n",
    "difficulty_data = {\"Category\": [], \"Easy\": [], \"Medium\": [], \"Hard\": []}\n",
    "\n",
    "for file in files:\n",
    "    category = file.split(\"_\")[1].replace(\".json\", \"\").replace(\"_\", \" \")\n",
    "    questions = load_questions_from_file(file)\n",
    "    difficulties = [q.difficulty for q in questions]\n",
    "    counts = Counter(difficulties)\n",
    "\n",
    "    difficulty_data[\"Category\"].append(category)\n",
    "    difficulty_data[\"Easy\"].append(counts.get(\"easy\", 0))\n",
    "    difficulty_data[\"Medium\"].append(counts.get(\"medium\", 0))\n",
    "    difficulty_data[\"Hard\"].append(counts.get(\"hard\", 0))\n",
    "\n",
    "df = pd.DataFrame(difficulty_data)\n",
    "\n",
    "df.set_index(\"Category\", inplace=True)\n",
    "\n",
    "df.plot(kind=\"bar\", stacked=True, figsize=(10, 6))\n",
    "plt.title(\"Difficulty Counts by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Difficulty\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "94163c8b4233423b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Promting questions from GPT4o\n",
    "\n",
    "- give me 40 questions about history in easy difficulty, with each a correct answer and three incorrect answers\n",
    "- give me 28 more\n",
    "- give me 40 questions about history in medium difficulty, with each a correct answer and three incorrect answers\n",
    "- give me 40 more\n",
    "- give me 40 more\n",
    "- give me 40 more\n",
    "- give me 6 more\n",
    "- give me 40 questions about history in hard difficulty, with each a correct answer and three incorrect answers\n",
    "- give me 40 more\n",
    "\n",
    "- give me 40 questions about general knowledge in easy difficulty, with each a correct answer and three incorrect answers\n",
    "- give me 40 more\n",
    "\n",
    "...............\n"
   ],
   "id": "763fe1a91b7a95cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Compare text similarity",
   "id": "6773bd639c47e2da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cosine Similarity measures the similarity between two vectors. It ranges from -1 to 1, where 1 means the vectors are identical, 0 means they are orthogonal (no similarity), and -1 means they are diametrically opposed.",
   "id": "fee1f0279003898a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the largest country in the world?\",\n",
    "    \"Who was the first president of the United States?\",\n",
    "    \"Which planet is known as the Red Planet?\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(questions)\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_sim)"
   ],
   "id": "5364da9a36ad48a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Average Cosine Similarity is the mean similarity score across all pairs of items in the dataset.\n",
    "\n",
    "To determine diversity, we actually want to measure dissimilarity, so we subtract the average similarity from 1. The formula\n",
    "`1 − avg_simi` essentially converts a similarity measure into a diversity (or dissimilarity) measure.\n",
    "- When avg_simi is close to 1 (high similarity), `1 − avg_simi` will be close to 0, indicating low diversity.\n",
    "- When avg_simi is close to 0 (low similarity), `1 − avg_simi` will be close to 1, indicating high diversity."
   ],
   "id": "f879ce4f35e61bf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from file_utils import load_questions_from_file\n",
    "from metric_utils import calculate_diversity\n",
    "\n",
    "otdbFiles = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4oFiles = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "data = {\n",
    "    \"Category\": [],\n",
    "    \"OTDB Diversity\": [],\n",
    "    \"GPT4o Diversity\": [],\n",
    "}\n",
    "\n",
    "for file_list, source in zip([otdbFiles, gpt4oFiles], [\"OTDB\", \"GPT4o\"]):\n",
    "    for file in file_list:\n",
    "        questions = load_questions_from_file(file)\n",
    "        category = file.split(\"_\")[1].replace(\".json\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "        questions_text = [q.question for q in questions]\n",
    "\n",
    "        diversity = calculate_diversity(questions_text)\n",
    "\n",
    "        data[\"Category\"].append(category)\n",
    "        if source == \"OTDB\":\n",
    "            data[\"OTDB Diversity\"].append(diversity)\n",
    "            data[\"GPT4o Diversity\"].append(None)\n",
    "        else:\n",
    "            data[\"GPT4o Diversity\"].append(diversity)\n",
    "            data[\"OTDB Diversity\"].append(None)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df.plot(kind=\"bar\", x=\"Category\", y=[\"OTDB Diversity\", \"GPT4o Diversity\"], ax=ax, stacked=False)\n",
    "\n",
    "plt.title(\"Diversity Comparison by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Diversity Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "51d0e2a83511565b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the cosine similarity across the full datasets (OTDB and GPT-4 separately).",
   "id": "48320c2c8fe7c32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from file_utils import load_questions_from_file\n",
    "\n",
    "otdb_files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4o_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "def calculate_cosine_similarity_for_files(files):\n",
    "    all_questions = []\n",
    "    for file in files:\n",
    "        questions = load_questions_from_file(file)\n",
    "        for question in questions:\n",
    "            all_questions.append(question.question)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "otdb_similarity_matrix = calculate_cosine_similarity_for_files(otdb_files)\n",
    "gpt4o_similarity_matrix = calculate_cosine_similarity_for_files(gpt4o_files)\n",
    "\n",
    "def plot_similarity_heatmaps_side_by_side(similarity_matrix1, dataset_name1, similarity_matrix2, dataset_name2):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    sns.heatmap(similarity_matrix1, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[0])\n",
    "    axes[0].set_title(f\"Cosine Similarity Heatmap for {dataset_name1}\")\n",
    "    axes[0].set_xlabel(\"Questions\")\n",
    "    axes[0].set_ylabel(\"Questions\")\n",
    "\n",
    "    sns.heatmap(similarity_matrix2, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[1])\n",
    "    axes[1].set_title(f\"Cosine Similarity Heatmap for {dataset_name2}\")\n",
    "    axes[1].set_xlabel(\"Questions\")\n",
    "    axes[1].set_ylabel(\"Questions\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_similarity_heatmaps_side_by_side(otdb_similarity_matrix, \"OTDB Dataset\", gpt4o_similarity_matrix, \"GPT4o Dataset\")"
   ],
   "id": "a3f0543243657cf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the cosine similarity across the categories in the datasets.",
   "id": "19d2e3db7652e5df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "otdb_files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4o_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "def calculate_cosine_similarity_for_file(file):\n",
    "    questions = load_questions_from_file(file)\n",
    "    all_questions = [question.question for question in questions]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def plot_category_similarity_heatmaps(otdb_files, gpt4o_files, categories):\n",
    "    fig, axes = plt.subplots(len(categories), 2, figsize=(16, 6 * len(categories)))\n",
    "\n",
    "    for i, (otdb_file, gpt4o_file, category) in enumerate(zip(otdb_files, gpt4o_files, categories)):\n",
    "        # Calculate similarity matrices\n",
    "        otdb_similarity_matrix = calculate_cosine_similarity_for_file(otdb_file)\n",
    "        gpt4o_similarity_matrix = calculate_cosine_similarity_for_file(gpt4o_file)\n",
    "\n",
    "        # Plot OTDB heatmap\n",
    "        sns.heatmap(otdb_similarity_matrix, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f\"OTDB: {category}\")\n",
    "        axes[i, 0].set_xlabel(\"Questions\")\n",
    "        axes[i, 0].set_ylabel(\"Questions\")\n",
    "\n",
    "        # Plot GPT4o heatmap\n",
    "        sns.heatmap(gpt4o_similarity_matrix, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[i, 1])\n",
    "        axes[i, 1].set_title(f\"GPT4o: {category}\")\n",
    "        axes[i, 1].set_xlabel(\"Questions\")\n",
    "        axes[i, 1].set_ylabel(\"Questions\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define categories\n",
    "categories = [\"History\", \"General Knowledge\", \"Science & Nature\"]\n",
    "\n",
    "# Plot heatmaps for each category\n",
    "plot_category_similarity_heatmaps(otdb_files, gpt4o_files, categories)"
   ],
   "id": "7c076ad90835e00f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Text metrics\n",
    "\n",
    "The word count is determined by extracting and counting all words in a question, while the sentence length is calculated by dividing the total number of characters by the number of words. To measure unique word count, distinct words are identified in each question after removing common English stop words. For each category, the total values are averaged across all questions, and these averages are used to compare the datasets visually through bar charts."
   ],
   "id": "98bbbe2e27b2da23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from file_utils import load_questions_from_file\n",
    "from metric_utils import calculate_word_count_and_sentence_length\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "def calculate_unique_words(questions):\n",
    "    all_words = []\n",
    "    for question in questions:\n",
    "\n",
    "        words = re.findall(r'\\b\\w+\\b', question.lower())\n",
    "\n",
    "        filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "        all_words.extend(filtered_words)\n",
    "    return len(set(all_words))\n",
    "\n",
    "otdb_files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4o_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "data = {\n",
    "    \"Category\": [],\n",
    "    \"Word Count\": [],\n",
    "    \"Sentence Length\": [],\n",
    "    \"Unique Word Count\": [],\n",
    "    \"Data Source\": []\n",
    "}\n",
    "\n",
    "for file_list, source in zip([otdb_files, gpt4o_files], [\"OTDB\", \"GPT4o\"]):\n",
    "    for file in file_list:\n",
    "        questions = load_questions_from_file(file)\n",
    "        category = file.split(\"_\")[1].replace(\".json\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "        total_word_count = 0\n",
    "        total_sentence_length = 0\n",
    "        total_unique_word_count = 0\n",
    "        question_count = len(questions)\n",
    "\n",
    "        for question in questions:\n",
    "            word_count, sentence_length = calculate_word_count_and_sentence_length(question.question)\n",
    "            total_word_count += word_count\n",
    "            total_sentence_length += sentence_length\n",
    "            total_unique_word_count += calculate_unique_words([question.question])\n",
    "\n",
    "        avg_word_count = total_word_count / question_count\n",
    "        avg_sentence_length = total_sentence_length / question_count\n",
    "        avg_unique_word_count = total_unique_word_count / question_count\n",
    "\n",
    "        data[\"Category\"].append(category)\n",
    "        data[\"Word Count\"].append(avg_word_count)\n",
    "        data[\"Sentence Length\"].append(avg_sentence_length)\n",
    "        data[\"Unique Word Count\"].append(avg_unique_word_count)\n",
    "        data[\"Data Source\"].append(source)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "df_pivot_word_count = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Word Count\", aggfunc='mean')\n",
    "df_pivot_word_count.plot(kind=\"bar\", ax=axes[0], stacked=False)\n",
    "axes[0].set_title(\"Average Word Count by Category\")\n",
    "axes[0].set_xlabel(\"Category\")\n",
    "axes[0].set_ylabel(\"Average Word Count\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "df_pivot_sentence_length = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Sentence Length\", aggfunc='mean')\n",
    "df_pivot_sentence_length.plot(kind=\"bar\", ax=axes[1], stacked=False)\n",
    "axes[1].set_title(\"Average Sentence Length by Category\")\n",
    "axes[1].set_xlabel(\"Category\")\n",
    "axes[1].set_ylabel(\"Average Sentence Length\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "df_pivot_unique_word_count = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Unique Word Count\", aggfunc='mean')\n",
    "df_pivot_unique_word_count.plot(kind=\"bar\", ax=axes[2], stacked=False)\n",
    "axes[2].set_title(\"Average Unique Word Count by Category\")\n",
    "axes[2].set_xlabel(\"Category\")\n",
    "axes[2].set_ylabel(\"Average Unique Word Count\")\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "de05c1ca7f762cff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Improve Promting\n",
    "\n",
    "Strategies to improve prompting\n",
    "1. Format (Markdown, YAML, etc)\n",
    "2. Act in a Role\n",
    "3. Be specific (language, emotions, etc.)\n",
    "4. Add information and data\n",
    "5. Use examples\n",
    "6. Add a process of analysis (“thinking”)\n",
    "7. Constraints (What should or shouldn’t the model do)\n",
    "8. (Use an english prompt)\n",
    "9. Redo with small changes\n",
    "\n",
    "Taking some pieces of these instructions, improve the way GPT4o creates questions.\n",
    "\n",
    "```\n",
    "## Instruction:\n",
    "You are an expert in the topic \"History\". Your task is to generate diverse trivia quiz questions, with three difficulty levels: \"easy,\" \"medium,\" and \"hard\".\n",
    "Each question consists of one correct answer and three plausible wrong answers.\n",
    "\n",
    "## Example Format:\n",
    "Who was the first President of the United States?;\n",
    "Correct: George Washington;\n",
    "Incorrect: John Adams; Thomas Jefferson; Benjamin Franklin;\n",
    "\n",
    "User Input:\n",
    "Generate 20 easy questions\n",
    "```\n",
    "\n",
    "Comparing the cosine similarity between simple and advanced prompting."
   ],
   "id": "509361e341109011"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "otdb_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "gpt4o_files =  [\"GPT4o_HISTORY_PROMT2.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT2.json\", \"GPT4o_SCIENCE_NATURE_PROMT2.json\"]\n",
    "\n",
    "def calculate_cosine_similarity_for_file(file):\n",
    "    questions = load_questions_from_file(file)\n",
    "    all_questions = [question.question for question in questions]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def plot_category_similarity_heatmaps(gpt4o_files_simple, gpt4o_files, categories):\n",
    "    fig, axes = plt.subplots(len(categories), 2, figsize=(16, 6 * len(categories)))\n",
    "\n",
    "    for i, (gpt4o_file_simple, gpt4o_file, category) in enumerate(zip(gpt4o_files_simple, gpt4o_files, categories)):\n",
    "        # Calculate similarity matrices\n",
    "        gpt4o_simple_similarity_matrix = calculate_cosine_similarity_for_file(gpt4o_file_simple)\n",
    "        gpt4o_similarity_matrix = calculate_cosine_similarity_for_file(gpt4o_file)\n",
    "\n",
    "        # Plot OTDB heatmap\n",
    "        sns.heatmap(gpt4o_simple_similarity_matrix, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f\"GPT4o (1): {category}\")\n",
    "        axes[i, 0].set_xlabel(\"Questions\")\n",
    "        axes[i, 0].set_ylabel(\"Questions\")\n",
    "\n",
    "        # Plot GPT4o heatmap\n",
    "        sns.heatmap(gpt4o_similarity_matrix, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[i, 1])\n",
    "        axes[i, 1].set_title(f\"GPT4o (2): {category}\")\n",
    "        axes[i, 1].set_xlabel(\"Questions\")\n",
    "        axes[i, 1].set_ylabel(\"Questions\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define categories\n",
    "categories = [\"History\", \"General Knowledge\", \"Science & Nature\"]\n",
    "\n",
    "# Plot heatmaps for each category\n",
    "plot_category_similarity_heatmaps(otdb_files, gpt4o_files, categories)"
   ],
   "id": "305a4c2f354c8a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Updated text metrics",
   "id": "9b6174b83ee7089b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from file_utils import load_questions_from_file\n",
    "from metric_utils import calculate_word_count_and_sentence_length\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "def calculate_unique_words(questions):\n",
    "    all_words = []\n",
    "    for question in questions:\n",
    "        words = re.findall(r'\\b\\w+\\b', question.lower())\n",
    "        filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "        all_words.extend(filtered_words)\n",
    "    return len(set(all_words))\n",
    "\n",
    "# File paths for OTDB, GPT4o (Prompt1) and GPT4o (Prompt2)\n",
    "otdb_files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4o_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "gpt4o_improved_files = [\"GPT4o_HISTORY_PROMT2.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT2.json\", \"GPT4o_SCIENCE_NATURE_PROMT2.json\"]\n",
    "\n",
    "data = {\n",
    "    \"Category\": [],\n",
    "    \"Word Count\": [],\n",
    "    \"Sentence Length\": [],\n",
    "    \"Unique Word Count\": [],\n",
    "    \"Data Source\": []\n",
    "}\n",
    "\n",
    "# Loop over OTDB, GPT4o Prompt1, and GPT4o Prompt2 datasets\n",
    "for file_list, source in zip([otdb_files, gpt4o_files, gpt4o_improved_files], [\"OTDB\", \"GPT4o Prompt1\", \"GPT4o Prompt2\"]):\n",
    "    for file in file_list:\n",
    "        questions = load_questions_from_file(file)\n",
    "        category = file.split(\"_\")[1].replace(\".json\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "        total_word_count = 0\n",
    "        total_sentence_length = 0\n",
    "        total_unique_word_count = 0\n",
    "        question_count = len(questions)\n",
    "\n",
    "        for question in questions:\n",
    "            word_count, sentence_length = calculate_word_count_and_sentence_length(question.question)\n",
    "            total_word_count += word_count\n",
    "            total_sentence_length += sentence_length\n",
    "            total_unique_word_count += calculate_unique_words([question.question])\n",
    "\n",
    "        avg_word_count = total_word_count / question_count\n",
    "        avg_sentence_length = total_sentence_length / question_count\n",
    "        avg_unique_word_count = total_unique_word_count / question_count\n",
    "\n",
    "        # Store the calculated data\n",
    "        data[\"Category\"].append(category)\n",
    "        data[\"Word Count\"].append(avg_word_count)\n",
    "        data[\"Sentence Length\"].append(avg_sentence_length)\n",
    "        data[\"Unique Word Count\"].append(avg_unique_word_count)\n",
    "        data[\"Data Source\"].append(source)\n",
    "\n",
    "# Convert data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "# Word Count Bar Chart\n",
    "df_pivot_word_count = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Word Count\", aggfunc='mean')\n",
    "df_pivot_word_count.plot(kind=\"bar\", ax=axes[0], stacked=False)\n",
    "axes[0].set_title(\"Average Word Count by Category\")\n",
    "axes[0].set_xlabel(\"Category\")\n",
    "axes[0].set_ylabel(\"Average Word Count\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sentence Length Bar Chart\n",
    "df_pivot_sentence_length = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Sentence Length\", aggfunc='mean')\n",
    "df_pivot_sentence_length.plot(kind=\"bar\", ax=axes[1], stacked=False)\n",
    "axes[1].set_title(\"Average Sentence Length by Category\")\n",
    "axes[1].set_xlabel(\"Category\")\n",
    "axes[1].set_ylabel(\"Average Sentence Length\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Unique Word Count Bar Chart\n",
    "df_pivot_unique_word_count = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Unique Word Count\", aggfunc='mean')\n",
    "df_pivot_unique_word_count.plot(kind=\"bar\", ax=axes[2], stacked=False)\n",
    "axes[2].set_title(\"Average Unique Word Count by Category\")\n",
    "axes[2].set_xlabel(\"Category\")\n",
    "axes[2].set_ylabel(\"Average Unique Word Count\")\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "97c82386c48f904c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Findings\n",
    "\n",
    "Higher Diversity in OTDB:\n",
    "\n",
    "The OTDB dataset shows significantly more diversity across multiple metrics such as word count, average sentence length, and unique word count per category. This suggests that the human-created questions in OTDB are likely more varied and complex in terms of phrasing and vocabulary. In contrast, GPT-generated questions are more concise, reflecting the LLM's focus on brevity and clarity in question generation.\n",
    "\n",
    "Limited Improvement with Advanced Prompting:\n",
    "\n",
    "The advanced prompting (GPT4o Prompt2) did not show much improvement over the simple prompting (GPT4o Prompt1). While advanced techniques might improve question relevance or make them slightly more sophisticated, the core attributes such as sentence length and diversity were only marginally affected."
   ],
   "id": "296442fd0ba063e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "https://github.com/lechld/gpt-quiz-generation"
   ],
   "id": "38703926ab93681d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Add-On\n",
    "\n",
    "The results of comparing cosine similarity across the datasets and categories lead to the conclusion that there is high diversity. However, this metric is not perfect, as the removal of stop words results in very short phrases being compared. To verify this assumption, a check on a smaller dataset can be performed. In this case, we use the first three questions from the basic general knowledge GPT4o prompt.\n"
   ],
   "id": "86f7d71103c0dd9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How many continents are there in the world?\",\n",
    "    \"What is the largest planet in our solar system?\"\n",
    "]\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_with_stopwords = vectorizer.fit_transform(questions)\n",
    "cosine_sim_with_stopwords = cosine_similarity(tfidf_matrix_with_stopwords)\n",
    "\n",
    "questions_no_stopwords = [remove_stop_words(q) for q in questions]\n",
    "tfidf_matrix_without_stopwords = vectorizer.fit_transform(questions_no_stopwords)\n",
    "cosine_sim_without_stopwords = cosine_similarity(tfidf_matrix_without_stopwords)\n",
    "\n",
    "print(\"Cosine Similarity with Stop Words:\")\n",
    "print(cosine_sim_with_stopwords)\n",
    "\n",
    "print(\"\\nCosine Similarity without Stop Words:\")\n",
    "print(cosine_sim_without_stopwords)"
   ],
   "id": "ea57f6bbcb4b089c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above results clearly show that removing stop words leads to much less diversity. Now, let's compare the cosine similarity of the basic GPT4o questions with and without stop word removal.",
   "id": "306d25fe9f7eedc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from file_utils import load_questions_from_file\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpt4o_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "def calculate_cosine_similarity_for_files(files, stop_words):\n",
    "    all_questions = []\n",
    "    for file in files:\n",
    "        questions = load_questions_from_file(file)\n",
    "        for question in questions:\n",
    "            all_questions.append(question.question)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "without_stop_words = calculate_cosine_similarity_for_files(gpt4o_files, 'english')\n",
    "with_stop_words = calculate_cosine_similarity_for_files(gpt4o_files, None)\n",
    "\n",
    "def calculate_average_similarity(similarity_matrix):\n",
    "    triu_matrix = np.triu(similarity_matrix, k=1)\n",
    "    avg_similarity = np.sum(triu_matrix) / (triu_matrix.shape[0] * (triu_matrix.shape[0] - 1) / 2)  # Calculate the average\n",
    "    return avg_similarity\n",
    "\n",
    "avg_without_stop_words = calculate_average_similarity(without_stop_words)\n",
    "avg_with_stop_words = calculate_average_similarity(with_stop_words)\n",
    "\n",
    "print(f\"Average Cosine Similarity without Stop Words: {avg_without_stop_words:.4f}\")\n",
    "print(f\"Average Cosine Similarity with Stop Words: {avg_with_stop_words:.4f}\")\n",
    "\n",
    "def plot_similarity_heatmaps_side_by_side(similarity_matrix1, dataset_name1, similarity_matrix2, dataset_name2):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    sns.heatmap(similarity_matrix1, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[0])\n",
    "    axes[0].set_title(f\"Cosine Similarity without Stop Words - {dataset_name1}\")\n",
    "    axes[0].set_xlabel(\"Questions\")\n",
    "    axes[0].set_ylabel(\"Questions\")\n",
    "\n",
    "    sns.heatmap(similarity_matrix2, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[1])\n",
    "    axes[1].set_title(f\"Cosine Similarity with Stop Words - {dataset_name2}\")\n",
    "    axes[1].set_xlabel(\"Questions\")\n",
    "    axes[1].set_ylabel(\"Questions\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_similarity_heatmaps_side_by_side(without_stop_words, \"GPT4o Dataset (without stop words)\", with_stop_words, \"GPT4o Dataset (with stop words)\")"
   ],
   "id": "17deb07bc0b63157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Although the cosine similarity heatmap of the dataset containing the stop words still show a high diversity, the similarity increases by a factor of approximately 5.\n",
    "\n",
    "### 7.1 Conclusion\n",
    "\n",
    "The low cosine similarity values for the questions, even with stop words included, are a result of short question lengths and the lack of semantic understanding. Cosine similarity is not ideal for capturing deeper semantic relationships between rephrased questions or sentences, and for better results, techniques that capture the meaning (e.g., word embeddings, BERT, or semantic analysis) would be more appropriate. This can be evaluated in future works.\n",
    "\n"
   ],
   "id": "6a6c8bd63e0d9db5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
