{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparing LLM generated questions with OTDB questions\n",
    "\n",
    "This project examines how static quiz questions from the Open Trivia Database (OTDB) differ from dynamically generated questions from a Large Language Model (LLM). Through systematic comparison, the goal is to determine whether LLMs can be a useful supplement or alternative to existing quiz databases."
   ],
   "id": "3eccb7c4621c81eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Categories and Questions from OTDB",
   "id": "e0553c2b711eef08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "from file_utils import load_questions_from_file\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "\n",
    "difficulty_data = {\"Category\": [], \"Easy\": [], \"Medium\": [], \"Hard\": []}\n",
    "\n",
    "for file in files:\n",
    "    category = file.split(\"_\")[1].replace(\".json\", \"\").replace(\"_\", \" \")\n",
    "    questions = load_questions_from_file(file)\n",
    "    difficulties = [q.difficulty for q in questions]\n",
    "    counts = Counter(difficulties)\n",
    "\n",
    "    difficulty_data[\"Category\"].append(category)\n",
    "    difficulty_data[\"Easy\"].append(counts.get(\"easy\", 0))\n",
    "    difficulty_data[\"Medium\"].append(counts.get(\"medium\", 0))\n",
    "    difficulty_data[\"Hard\"].append(counts.get(\"hard\", 0))\n",
    "\n",
    "df = pd.DataFrame(difficulty_data)\n",
    "\n",
    "df.set_index(\"Category\", inplace=True)\n",
    "\n",
    "df.plot(kind=\"bar\", stacked=True, figsize=(10, 6))\n",
    "plt.title(\"Difficulty Counts by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Difficulty\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "94163c8b4233423b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Promting questions from GPT4o\n",
    "\n",
    "- give me 40 questions about history in easy difficulty, with each a correct answer and three incorrect answers\n",
    "- give me 28 more\n",
    "- give me 40 questions about history in medium difficulty, with each a correct answer and three incorrect answers\n",
    "- give me 40 more\n",
    "- give me 40 more\n",
    "- give me 40 more\n",
    "- give me 6 more\n",
    "- give me 40 questions about history in hard difficulty, with each a correct answer and three incorrect answers\n",
    "- give me 40 more\n",
    "\n",
    "- give me 40 questions about general knowledge in easy difficulty, with each a correct answer and three incorrect answers\n",
    "- give me 40 more\n",
    "\n",
    "...............\n"
   ],
   "id": "763fe1a91b7a95cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Compare text similarity",
   "id": "6773bd639c47e2da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cosine Similarity measures the similarity between two vectors. It ranges from -1 to 1, where 1 means the vectors are identical, 0 means they are orthogonal (no similarity), and -1 means they are diametrically opposed.",
   "id": "fee1f0279003898a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the largest country in the world?\",\n",
    "    \"Who was the first president of the United States?\",\n",
    "    \"Which planet is known as the Red Planet?\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(questions)\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_sim)"
   ],
   "id": "5364da9a36ad48a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Average Cosine Similarity is the mean similarity score across all pairs of items in the dataset.\n",
    "\n",
    "To determine diversity, we actually want to measure dissimilarity, so we subtract the average similarity from 1. The formula\n",
    "`1 − avg_simi` essentially converts a similarity measure into a diversity (or dissimilarity) measure.\n",
    "- When avg_simi is close to 1 (high similarity), `1 − avg_simi` will be close to 0, indicating low diversity.\n",
    "- When avg_simi is close to 0 (low similarity), `1 − avg_simi` will be close to 1, indicating high diversity."
   ],
   "id": "f879ce4f35e61bf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from file_utils import load_questions_from_file\n",
    "from metric_utils import calculate_diversity\n",
    "\n",
    "otdbFiles = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4oFiles = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "data = {\n",
    "    \"Category\": [],\n",
    "    \"OTDB Diversity\": [],\n",
    "    \"GPT4o Diversity\": [],\n",
    "}\n",
    "\n",
    "for file_list, source in zip([otdbFiles, gpt4oFiles], [\"OTDB\", \"GPT4o\"]):\n",
    "    for file in file_list:\n",
    "        questions = load_questions_from_file(file)\n",
    "        category = file.split(\"_\")[1].replace(\".json\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "        questions_text = [q.question for q in questions]\n",
    "\n",
    "        diversity = calculate_diversity(questions_text)\n",
    "\n",
    "        data[\"Category\"].append(category)\n",
    "        if source == \"OTDB\":\n",
    "            data[\"OTDB Diversity\"].append(diversity)\n",
    "            data[\"GPT4o Diversity\"].append(None)\n",
    "        else:\n",
    "            data[\"GPT4o Diversity\"].append(diversity)\n",
    "            data[\"OTDB Diversity\"].append(None)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df.plot(kind=\"bar\", x=\"Category\", y=[\"OTDB Diversity\", \"GPT4o Diversity\"], ax=ax, stacked=False)\n",
    "\n",
    "plt.title(\"Diversity Comparison by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Diversity Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "51d0e2a83511565b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the cosine similarity across the full datasets (OTDB and GPT-4 separately).",
   "id": "48320c2c8fe7c32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from file_utils import load_questions_from_file\n",
    "otdb_files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4o_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "def calculate_cosine_similarity_for_files(files):\n",
    "    all_questions = []\n",
    "    for file in files:\n",
    "        questions = load_questions_from_file(file)\n",
    "        for question in questions:\n",
    "            all_questions.append(question.question)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "otdb_similarity_matrix = calculate_cosine_similarity_for_files(otdb_files)\n",
    "gpt4o_similarity_matrix = calculate_cosine_similarity_for_files(gpt4o_files)\n",
    "\n",
    "def plot_similarity_heatmaps_side_by_side(similarity_matrix1, dataset_name1, similarity_matrix2, dataset_name2):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    sns.heatmap(similarity_matrix1, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[0])\n",
    "    axes[0].set_title(f\"Cosine Similarity Heatmap for {dataset_name1}\")\n",
    "    axes[0].set_xlabel(\"Questions\")\n",
    "    axes[0].set_ylabel(\"Questions\")\n",
    "\n",
    "    sns.heatmap(similarity_matrix2, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[1])\n",
    "    axes[1].set_title(f\"Cosine Similarity Heatmap for {dataset_name2}\")\n",
    "    axes[1].set_xlabel(\"Questions\")\n",
    "    axes[1].set_ylabel(\"Questions\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_similarity_heatmaps_side_by_side(otdb_similarity_matrix, \"OTDB Dataset\", gpt4o_similarity_matrix, \"GPT4o Dataset\")"
   ],
   "id": "a3f0543243657cf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the cosine similarity across the categories in the datasets.",
   "id": "19d2e3db7652e5df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "otdb_files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4o_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "def calculate_cosine_similarity_for_file(file):\n",
    "    questions = load_questions_from_file(file)\n",
    "    all_questions = [question.question for question in questions]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def plot_category_similarity_heatmaps(otdb_files, gpt4o_files, categories):\n",
    "    fig, axes = plt.subplots(len(categories), 2, figsize=(16, 6 * len(categories)))\n",
    "\n",
    "    for i, (otdb_file, gpt4o_file, category) in enumerate(zip(otdb_files, gpt4o_files, categories)):\n",
    "        # Calculate similarity matrices\n",
    "        otdb_similarity_matrix = calculate_cosine_similarity_for_file(otdb_file)\n",
    "        gpt4o_similarity_matrix = calculate_cosine_similarity_for_file(gpt4o_file)\n",
    "\n",
    "        # Plot OTDB heatmap\n",
    "        sns.heatmap(otdb_similarity_matrix, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f\"OTDB: {category}\")\n",
    "        axes[i, 0].set_xlabel(\"Questions\")\n",
    "        axes[i, 0].set_ylabel(\"Questions\")\n",
    "\n",
    "        # Plot GPT4o heatmap\n",
    "        sns.heatmap(gpt4o_similarity_matrix, annot=False, cmap=\"coolwarm\", xticklabels=False, yticklabels=False, ax=axes[i, 1])\n",
    "        axes[i, 1].set_title(f\"GPT4o: {category}\")\n",
    "        axes[i, 1].set_xlabel(\"Questions\")\n",
    "        axes[i, 1].set_ylabel(\"Questions\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define categories\n",
    "categories = [\"History\", \"General Knowledge\", \"Science & Nature\"]\n",
    "\n",
    "# Plot heatmaps for each category\n",
    "plot_category_similarity_heatmaps(otdb_files, gpt4o_files, categories)"
   ],
   "id": "7c076ad90835e00f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Text metrics\n",
    "\n",
    "The word count is determined by extracting and counting all words in a question, while the sentence length is calculated by dividing the total number of characters by the number of words. To measure unique word count, distinct words are identified in each question after removing common English stop words. For each category, the total values are averaged across all questions, and these averages are used to compare the datasets visually through bar charts."
   ],
   "id": "98bbbe2e27b2da23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from file_utils import load_questions_from_file\n",
    "from metric_utils import calculate_word_count_and_sentence_length\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "def calculate_unique_words(questions):\n",
    "    all_words = []\n",
    "    for question in questions:\n",
    "\n",
    "        words = re.findall(r'\\b\\w+\\b', question.lower())\n",
    "\n",
    "        filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "        all_words.extend(filtered_words)\n",
    "    return len(set(all_words))\n",
    "\n",
    "otdb_files = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4o_files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "data = {\n",
    "    \"Category\": [],\n",
    "    \"Word Count\": [],\n",
    "    \"Sentence Length\": [],\n",
    "    \"Unique Word Count\": [],\n",
    "    \"Data Source\": []\n",
    "}\n",
    "\n",
    "for file_list, source in zip([otdb_files, gpt4o_files], [\"OTDB\", \"GPT4o\"]):\n",
    "    for file in file_list:\n",
    "        questions = load_questions_from_file(file)\n",
    "        category = file.split(\"_\")[1].replace(\".json\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "        total_word_count = 0\n",
    "        total_sentence_length = 0\n",
    "        total_unique_word_count = 0\n",
    "        question_count = len(questions)\n",
    "\n",
    "        for question in questions:\n",
    "            word_count, sentence_length = calculate_word_count_and_sentence_length(question.question)\n",
    "            total_word_count += word_count\n",
    "            total_sentence_length += sentence_length\n",
    "            total_unique_word_count += calculate_unique_words([question.question])\n",
    "\n",
    "        avg_word_count = total_word_count / question_count\n",
    "        avg_sentence_length = total_sentence_length / question_count\n",
    "        avg_unique_word_count = total_unique_word_count / question_count\n",
    "\n",
    "        data[\"Category\"].append(category)\n",
    "        data[\"Word Count\"].append(avg_word_count)\n",
    "        data[\"Sentence Length\"].append(avg_sentence_length)\n",
    "        data[\"Unique Word Count\"].append(avg_unique_word_count)\n",
    "        data[\"Data Source\"].append(source)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "df_pivot_word_count = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Word Count\", aggfunc='mean')\n",
    "df_pivot_word_count.plot(kind=\"bar\", ax=axes[0], stacked=False)\n",
    "axes[0].set_title(\"Average Word Count by Category\")\n",
    "axes[0].set_xlabel(\"Category\")\n",
    "axes[0].set_ylabel(\"Average Word Count\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "df_pivot_sentence_length = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Sentence Length\", aggfunc='mean')\n",
    "df_pivot_sentence_length.plot(kind=\"bar\", ax=axes[1], stacked=False)\n",
    "axes[1].set_title(\"Average Sentence Length by Category\")\n",
    "axes[1].set_xlabel(\"Category\")\n",
    "axes[1].set_ylabel(\"Average Sentence Length\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "df_pivot_unique_word_count = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=\"Unique Word Count\", aggfunc='mean')\n",
    "df_pivot_unique_word_count.plot(kind=\"bar\", ax=axes[2], stacked=False)\n",
    "axes[2].set_title(\"Average Unique Word Count by Category\")\n",
    "axes[2].set_xlabel(\"Category\")\n",
    "axes[2].set_ylabel(\"Average Unique Word Count\")\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "de05c1ca7f762cff",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
