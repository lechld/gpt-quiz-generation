{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from file_utils import load_questions_from_file\n",
    "from metric_utils import calculate_word_count_and_sentence_length\n",
    "\n",
    "\n",
    "otdbFiles = [\"OTDB_HISTORY.json\", \"OTDB_GENERAL_KNOWLEDGE.json\", \"OTDB_SCIENCE_NATURE.json\"]\n",
    "gpt4oPromt1Files = [\"GPT4o_HISTORY_PROMT1.json\", \"GPT4o_GENERAL_KNOWLEDGE_PROMT1.json\", \"GPT4o_SCIENCE_NATURE_PROMT1.json\"]\n",
    "\n",
    "data = {\n",
    "    \"Category\": [],\n",
    "    \"Word Count\": [],\n",
    "    \"Sentence Length\": [],\n",
    "    \"Data Source\": []  # OTDB or GPT4o\n",
    "}\n",
    "\n",
    "for file_list, source in zip([otdbFiles, gpt4oPromt1Files], [\"OTDB\", \"GPT4o\"]):\n",
    "    for file in file_list:\n",
    "        questions = load_questions_from_file(file)\n",
    "        category = file.split(\"_\")[1].replace(\".json\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "        total_word_count = 0\n",
    "        total_sentence_length = 0\n",
    "        question_count = len(questions)\n",
    "\n",
    "        for question in questions:\n",
    "            word_count, sentence_length = calculate_word_count_and_sentence_length(question.question)\n",
    "            total_word_count += word_count\n",
    "            total_sentence_length += sentence_length\n",
    "\n",
    "        avg_word_count = total_word_count / question_count\n",
    "        avg_sentence_length = total_sentence_length / question_count\n",
    "\n",
    "        data[\"Category\"].append(category)\n",
    "        data[\"Word Count\"].append(avg_word_count)\n",
    "        data[\"Sentence Length\"].append(avg_sentence_length)\n",
    "        data[\"Data Source\"].append(source)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df_pivot = df.pivot_table(index=\"Category\", columns=\"Data Source\", values=[\"Word Count\", \"Sentence Length\"], aggfunc='mean')\n",
    "df_pivot.plot(kind=\"bar\", ax=ax)\n",
    "\n",
    "plt.title(\"Word Count and Sentence Length Comparison by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Average Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GPT4o Prompt1 seems to generate more concise questions compared to OTDB, which might make the GPT-generated questions easier to process. However, this could also indicate a trade-off between conciseness and potential complexity or depth of information.",
   "id": "ad2f02b92c9dc879"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
